{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any, Optional, Union\n",
        "from dataclasses import dataclass, field\n",
        "import logging\n",
        "\n",
        "# ML imports\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, QuantileTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import joblib\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "\n",
        "# Configuration & Setup\n",
        "\n",
        "@dataclass\n",
        "class ROIEngineConfig:\n",
        "    \"\"\"Advanced configuration for ROI Engine\"\"\"\n",
        "    # Data paths\n",
        "    base_dir: str = \"./roi_engine_project\"\n",
        "    data_filename: str = \"synthetic_cre_dataset.csv\"\n",
        "\n",
        "    # Model settings\n",
        "    target: str = \"total_return\"\n",
        "    date_col: str = \"date\"\n",
        "    id_cols: List[str] = field(default_factory=lambda: [\"market\", \"asset_type\"])\n",
        "\n",
        "    # Cross-validation\n",
        "    n_splits: int = 5\n",
        "    test_size_ratio: float = 0.2\n",
        "\n",
        "    # Model parameters\n",
        "    random_state: int = 42\n",
        "    n_jobs: int = -1\n",
        "\n",
        "    # Feature engineering\n",
        "    enable_advanced_features: bool = True\n",
        "    rolling_windows: List[int] = field(default_factory=lambda: [3, 6, 12])\n",
        "    lag_periods: List[int] = field(default_factory=lambda: [1, 2, 3, 6])\n",
        "\n",
        "    # Model selection\n",
        "    models_to_try: List[str] = field(default_factory=lambda: ['gradient_boosting', 'random_forest', 'ridge'])\n",
        "    enable_hyperparameter_tuning: bool = True\n",
        "\n",
        "    # Output settings\n",
        "    save_plots: bool = True\n",
        "    plot_format: str = \"png\"\n",
        "    plot_dpi: int = 300\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.data_dir = Path(self.base_dir) / \"data\"\n",
        "        self.artifacts_dir = Path(self.base_dir) / \"artifacts\"\n",
        "        self.plots_dir = self.artifacts_dir / \"plots\"\n",
        "\n",
        "        # Create directories\n",
        "        for dir_path in [self.data_dir, self.artifacts_dir, self.plots_dir]:\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.data_path = self.data_dir / self.data_filename\n",
        "\n",
        "\n",
        "# Logging Setup\n",
        "\n",
        "def setup_logging(config: ROIEngineConfig) -> logging.Logger:\n",
        "    \"\"\"Setup logging configuration\"\"\"\n",
        "    log_file = config.artifacts_dir / \"roi_engine.log\"\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    logger = logging.getLogger('ROI_Engine')\n",
        "    return logger\n",
        "\n",
        "\n",
        "# Enhanced Data Generation\n",
        "\n",
        "class SyntheticDataGenerator:\n",
        "    \"\"\"Enhanced synthetic CRE data generator with realistic market dynamics\"\"\"\n",
        "\n",
        "    def __init__(self, config: ROIEngineConfig, logger: logging.Logger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "    def generate_macroeconomic_data(self, n_periods: int, start_date: str = \"2016-01-01\") -> pd.DataFrame:\n",
        "        \"\"\"Generate realistic macroeconomic time series\"\"\"\n",
        "        np.random.seed(self.config.random_state)\n",
        "\n",
        "        dates = pd.date_range(start_date, periods=n_periods, freq=\"M\")\n",
        "        time_trend = np.linspace(0, 8, n_periods)\n",
        "\n",
        "        # More realistic macro indicators with autocorrelation\n",
        "        inflation_base = 2.0 + 0.8 * np.sin(time_trend) + np.random.normal(0, 0.3, n_periods)\n",
        "        inflation = np.cumsum(np.random.normal(0, 0.1, n_periods)) + inflation_base\n",
        "        inflation = np.clip(inflation, 0.5, 6.0)\n",
        "\n",
        "        fed_rate_changes = np.random.normal(0, 0.05, n_periods)\n",
        "        fed_rate_changes[::24] += np.random.normal(0, 0.3, len(fed_rate_changes[::24]))  # Policy changes\n",
        "        fed_rate = np.clip(np.cumsum(fed_rate_changes) + 1.5, 0.1, 8.0)\n",
        "\n",
        "        gdp_base = 2.5 + 0.5 * np.cos(time_trend * 1.2)\n",
        "        gdp_shocks = np.random.choice([0, 0, 0, 0, -2, 1.5], n_periods, p=[0.7, 0.1, 0.1, 0.05, 0.03, 0.02])\n",
        "        gdp_growth = gdp_base + gdp_shocks + np.random.normal(0, 0.2, n_periods)\n",
        "        gdp_growth = np.clip(gdp_growth, -3.0, 6.0)\n",
        "\n",
        "        # Employment and credit conditions\n",
        "        unemployment = np.clip(6.0 - 0.3 * gdp_growth + np.random.normal(0, 0.3, n_periods), 3.0, 12.0)\n",
        "        credit_spread = np.clip(1.5 + 0.2 * (fed_rate - 2.0) - 0.1 * gdp_growth +\n",
        "                               np.random.normal(0, 0.2, n_periods), 0.5, 4.0)\n",
        "\n",
        "        return pd.DataFrame({\n",
        "            'date': dates,\n",
        "            'inflation': inflation,\n",
        "            'fed_rate': fed_rate,\n",
        "            'gdp_growth': gdp_growth,\n",
        "            'unemployment': unemployment,\n",
        "            'credit_spread': credit_spread\n",
        "        })\n",
        "\n",
        "    def generate_market_data(self, macro_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Generate market-specific real estate data\"\"\"\n",
        "        n_periods = len(macro_df)\n",
        "\n",
        "        # Market definitions with more granular characteristics\n",
        "        market_params = {\n",
        "            \"NYC\": {\"base_rent\": 85, \"volatility\": 0.15, \"liquidity\": 0.9, \"barrier_to_entry\": 0.8},\n",
        "            \"LA\": {\"base_rent\": 55, \"volatility\": 0.18, \"liquidity\": 0.7, \"barrier_to_entry\": 0.6},\n",
        "            \"SF\": {\"base_rent\": 75, \"volatility\": 0.22, \"liquidity\": 0.6, \"barrier_to_entry\": 0.9},\n",
        "            \"CHI\": {\"base_rent\": 40, \"volatility\": 0.12, \"liquidity\": 0.8, \"barrier_to_entry\": 0.4},\n",
        "            \"DAL\": {\"base_rent\": 35, \"volatility\": 0.16, \"liquidity\": 0.7, \"barrier_to_entry\": 0.3},\n",
        "            \"BOS\": {\"base_rent\": 65, \"volatility\": 0.14, \"liquidity\": 0.6, \"barrier_to_entry\": 0.7},\n",
        "            \"SEA\": {\"base_rent\": 60, \"volatility\": 0.20, \"liquidity\": 0.5, \"barrier_to_entry\": 0.6}\n",
        "        }\n",
        "\n",
        "        asset_type_params = {\n",
        "            \"Multifamily\": {\"alpha\": 1.15, \"beta_gdp\": 0.3, \"beta_unemployment\": -0.4, \"cyclicality\": 0.7},\n",
        "            \"Office\": {\"alpha\": 1.0, \"beta_gdp\": 0.5, \"beta_unemployment\": -0.6, \"cyclicality\": 1.2},\n",
        "            \"Industrial\": {\"alpha\": 1.05, \"beta_gdp\": 0.4, \"beta_unemployment\": -0.2, \"cyclicality\": 0.8},\n",
        "            \"Retail\": {\"alpha\": 0.95, \"beta_gdp\": 0.6, \"beta_unemployment\": -0.8, \"cyclicality\": 1.5},\n",
        "            \"Hotel\": {\"alpha\": 1.3, \"beta_gdp\": 0.8, \"beta_unemployment\": -1.0, \"cyclicality\": 2.0}\n",
        "        }\n",
        "\n",
        "        records = []\n",
        "\n",
        "        for _, row in macro_df.iterrows():\n",
        "            # Generate multiple properties per period for more data\n",
        "            n_properties = np.random.randint(8, 15)\n",
        "\n",
        "            for _ in range(n_properties):\n",
        "                market = np.random.choice(list(market_params.keys()))\n",
        "                asset_type = np.random.choice(list(asset_type_params.keys()))\n",
        "\n",
        "                mp = market_params[market]\n",
        "                atp = asset_type_params[asset_type]\n",
        "\n",
        "                # Property characteristics\n",
        "                sqft = np.random.randint(25_000, 500_000)\n",
        "                age = np.random.randint(1, 50)\n",
        "                quality_score = np.random.uniform(0.6, 1.0)\n",
        "\n",
        "                # Dynamic rent calculation\n",
        "                base_rent = mp[\"base_rent\"] * atp[\"alpha\"] * quality_score\n",
        "\n",
        "                # Economic sensitivity\n",
        "                gdp_effect = atp[\"beta_gdp\"] * (row[\"gdp_growth\"] - 2.5) / 2.5\n",
        "                unemployment_effect = atp[\"beta_unemployment\"] * (row[\"unemployment\"] - 6.0) / 6.0\n",
        "\n",
        "                market_volatility = mp[\"volatility\"] * atp[\"cyclicality\"]\n",
        "                rent_shock = np.random.normal(0, market_volatility)\n",
        "\n",
        "                rent_psf = base_rent * (1 + gdp_effect + unemployment_effect + rent_shock)\n",
        "                rent_psf = np.clip(rent_psf, base_rent * 0.5, base_rent * 2.0)\n",
        "\n",
        "                annual_rent = rent_psf * sqft\n",
        "\n",
        "                # Vacancy dynamics\n",
        "                base_vacancy = 5.0 + np.random.uniform(-1, 3)\n",
        "                vacancy_sensitivity = -unemployment_effect * 2 + (row[\"fed_rate\"] - 2.0) * 0.5\n",
        "                vacancy_rate = np.clip(base_vacancy + vacancy_sensitivity +\n",
        "                                     np.random.normal(0, 1.5), 1.0, 25.0)\n",
        "\n",
        "                effective_rent = annual_rent * (1 - vacancy_rate / 100.0)\n",
        "\n",
        "                # Operating expenses with inflation sensitivity\n",
        "                opex_base = 0.25 + 0.05 * np.random.rand()\n",
        "                opex_inflation = 0.5 * (row[\"inflation\"] - 2.0) / 2.0\n",
        "                opex_rate = np.clip(opex_base + opex_inflation, 0.15, 0.45)\n",
        "                opex = effective_rent * opex_rate\n",
        "\n",
        "                noi = effective_rent - opex\n",
        "\n",
        "                # Cap rates with risk premiums\n",
        "                base_cap = 4.5 + 0.5 * (1 - mp[\"liquidity\"]) + 0.3 * atp[\"cyclicality\"]\n",
        "                risk_premium = 0.3 * (row[\"fed_rate\"] - 2.0) + 0.2 * row[\"credit_spread\"]\n",
        "                market_risk = -0.1 * (row[\"gdp_growth\"] - 2.5) / 2.5\n",
        "\n",
        "                cap_rate = np.clip(base_cap + risk_premium + market_risk +\n",
        "                                  np.random.normal(0, 0.2), 3.0, 12.0)\n",
        "\n",
        "                value = noi / (cap_rate / 100.0) if cap_rate > 0 else 0\n",
        "\n",
        "                # Financing\n",
        "                ltv_base = 0.65\n",
        "                ltv_adjustment = -0.05 * (row[\"fed_rate\"] - 2.0) / 2.0 - 0.03 * row[\"credit_spread\"]\n",
        "                ltv = np.clip(ltv_base + ltv_adjustment + np.random.normal(0, 0.05), 0.4, 0.8)\n",
        "\n",
        "                loan_amt = value * ltv\n",
        "                coupon = (row[\"fed_rate\"] + row[\"credit_spread\"] + 1.5 +\n",
        "                         0.5 * atp[\"cyclicality\"] + np.random.normal(0, 0.3)) / 100.0\n",
        "                coupon = np.clip(coupon, 0.02, 0.12)\n",
        "\n",
        "                annual_debt_service = loan_amt * coupon\n",
        "\n",
        "                # Returns calculation with market dynamics\n",
        "                market_appreciation = (0.02 + 0.5 * (row[\"gdp_growth\"] - 2.5) / 2.5 -\n",
        "                                     0.3 * (row[\"fed_rate\"] - 2.0) / 2.0)\n",
        "                asset_specific = atp[\"beta_gdp\"] * (row[\"gdp_growth\"] - 2.5) / 10.0\n",
        "                idiosyncratic = np.random.normal(0, market_volatility * 0.5)\n",
        "\n",
        "                appreciation_rate = market_appreciation + asset_specific + idiosyncratic\n",
        "                value_next = value * (1 + appreciation_rate)\n",
        "\n",
        "                equity = np.maximum(value - loan_amt, 1e-6)\n",
        "                cash_flow = noi - annual_debt_service\n",
        "                total_return = (cash_flow + (value_next - value)) / equity\n",
        "\n",
        "                # Additional metrics\n",
        "                dscr = noi / annual_debt_service if annual_debt_service > 0 else np.inf\n",
        "                cash_on_cash = cash_flow / equity\n",
        "\n",
        "                records.append({\n",
        "                    \"date\": row[\"date\"],\n",
        "                    \"market\": market,\n",
        "                    \"asset_type\": asset_type,\n",
        "                    \"sqft\": sqft,\n",
        "                    \"age\": age,\n",
        "                    \"quality_score\": quality_score,\n",
        "                    \"rent_psf\": rent_psf,\n",
        "                    \"annual_rent\": annual_rent,\n",
        "                    \"vacancy_rate\": vacancy_rate,\n",
        "                    \"effective_rent\": effective_rent,\n",
        "                    \"opex_rate\": opex_rate,\n",
        "                    \"opex\": opex,\n",
        "                    \"noi\": noi,\n",
        "                    \"cap_rate\": cap_rate,\n",
        "                    \"value\": value,\n",
        "                    \"ltv\": ltv,\n",
        "                    \"loan_amt\": loan_amt,\n",
        "                    \"coupon\": coupon * 100.0,\n",
        "                    \"annual_debt_service\": annual_debt_service,\n",
        "                    \"equity\": equity,\n",
        "                    \"value_next\": value_next,\n",
        "                    \"total_return\": total_return,\n",
        "                    \"dscr\": dscr,\n",
        "                    \"cash_on_cash\": cash_on_cash,\n",
        "                    **{k: row[k] for k in [\"inflation\", \"fed_rate\", \"gdp_growth\", \"unemployment\", \"credit_spread\"]}\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "    def generate_dataset(self, n_periods: int = 120) -> pd.DataFrame:\n",
        "        \"\"\"Generate complete synthetic dataset\"\"\"\n",
        "        self.logger.info(f\"Generating synthetic dataset with {n_periods} periods\")\n",
        "\n",
        "        # Generate macroeconomic data\n",
        "        macro_df = self.generate_macroeconomic_data(n_periods)\n",
        "\n",
        "        # Generate market data\n",
        "        market_df = self.generate_market_data(macro_df)\n",
        "\n",
        "        self.logger.info(f\"Generated {len(market_df)} property records\")\n",
        "        return market_df\n",
        "\n",
        "\n",
        "# Advanced Feature Engineering\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Advanced feature engineering for real estate data\"\"\"\n",
        "\n",
        "    def __init__(self, config: ROIEngineConfig, logger: logging.Logger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "    def create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create comprehensive time-based features\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Basic time features\n",
        "        df[\"year\"] = df[self.config.date_col].dt.year\n",
        "        df[\"month\"] = df[self.config.date_col].dt.month\n",
        "        df[\"quarter\"] = df[self.config.date_col].dt.quarter\n",
        "        df[\"year_month\"] = df[self.config.date_col].dt.to_period('M').astype(str)\n",
        "\n",
        "        # Cyclical encoding for seasonal patterns\n",
        "        df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
        "        df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
        "        df[\"quarter_sin\"] = np.sin(2 * np.pi * df[\"quarter\"] / 4)\n",
        "        df[\"quarter_cos\"] = np.cos(2 * np.pi * df[\"quarter\"] / 4)\n",
        "\n",
        "        # Time since start\n",
        "        min_date = df[self.config.date_col].min()\n",
        "        df[\"months_since_start\"] = ((df[self.config.date_col] - min_date).dt.days / 30.44).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_lag_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create lagged features for key variables\"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values([self.config.date_col] + self.config.id_cols).reset_index(drop=True)\n",
        "\n",
        "        # Key variables to lag\n",
        "        lag_vars = [\n",
        "            \"noi\", \"value\", \"cap_rate\", \"vacancy_rate\", \"rent_psf\",\n",
        "            \"fed_rate\", \"inflation\", \"gdp_growth\", \"unemployment\", \"credit_spread\",\n",
        "            \"total_return\", \"effective_rent\", \"opex_rate\"\n",
        "        ]\n",
        "\n",
        "        # Create lags for each variable\n",
        "        for var in lag_vars:\n",
        "            if var in df.columns:\n",
        "                for lag in self.config.lag_periods:\n",
        "                    try:\n",
        "                        # Group by market and asset type for proper lagging\n",
        "                        lagged_values = df.groupby(self.config.id_cols, group_keys=False)[var].shift(lag)\n",
        "                        df[f\"{var}_lag{lag}\"] = lagged_values.values  # Extract values to avoid index issues\n",
        "                    except Exception as e:\n",
        "                        self.logger.warning(f\"Error creating lag feature {var}_lag{lag}: {str(e)}\")\n",
        "                        # Fallback: simple shift without grouping\n",
        "                        df[f\"{var}_lag{lag}\"] = df[var].shift(lag)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_rolling_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create rolling window statistics\"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values([self.config.date_col] + self.config.id_cols).reset_index(drop=True)\n",
        "\n",
        "        # Variables for rolling features\n",
        "        rolling_vars = [\n",
        "            \"total_return\", \"noi\", \"cap_rate\", \"vacancy_rate\", \"rent_psf\",\n",
        "            \"fed_rate\", \"gdp_growth\", \"inflation\"\n",
        "        ]\n",
        "\n",
        "        for var in rolling_vars:\n",
        "            if var in df.columns:\n",
        "                for window in self.config.rolling_windows:\n",
        "                    try:\n",
        "                        # Use transform to maintain proper alignment\n",
        "                        grouped = df.groupby(self.config.id_cols, group_keys=False)[var]\n",
        "\n",
        "                        # Rolling mean and std\n",
        "                        df[f\"{var}_roll_mean_{window}\"] = grouped.rolling(\n",
        "                            window=window, min_periods=1\n",
        "                        ).mean().reset_index(level=self.config.id_cols, drop=True)\n",
        "\n",
        "                        df[f\"{var}_roll_std_{window}\"] = grouped.rolling(\n",
        "                            window=window, min_periods=1\n",
        "                        ).std().reset_index(level=self.config.id_cols, drop=True)\n",
        "\n",
        "                        # Rolling min/max\n",
        "                        df[f\"{var}_roll_min_{window}\"] = grouped.rolling(\n",
        "                            window=window, min_periods=1\n",
        "                        ).min().reset_index(level=self.config.id_cols, drop=True)\n",
        "\n",
        "                        df[f\"{var}_roll_max_{window}\"] = grouped.rolling(\n",
        "                            window=window, min_periods=1\n",
        "                        ).max().reset_index(level=self.config.id_cols, drop=True)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        self.logger.warning(f\"Error creating rolling feature {var}_roll_*_{window}: {str(e)}\")\n",
        "                        # Fallback: simple rolling without grouping\n",
        "                        try:\n",
        "                            df[f\"{var}_roll_mean_{window}\"] = df[var].rolling(window=window, min_periods=1).mean()\n",
        "                            df[f\"{var}_roll_std_{window}\"] = df[var].rolling(window=window, min_periods=1).std()\n",
        "                            df[f\"{var}_roll_min_{window}\"] = df[var].rolling(window=window, min_periods=1).min()\n",
        "                            df[f\"{var}_roll_max_{window}\"] = df[var].rolling(window=window, min_periods=1).max()\n",
        "                        except Exception as e2:\n",
        "                            self.logger.warning(f\"Fallback rolling feature creation also failed for {var}: {str(e2)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create interaction and derived features\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Economic interaction terms\n",
        "        if all(col in df.columns for col in [\"gdp_growth\", \"fed_rate\"]):\n",
        "            df[\"gdp_fed_interaction\"] = df[\"gdp_growth\"] * df[\"fed_rate\"]\n",
        "\n",
        "        if all(col in df.columns for col in [\"unemployment\", \"inflation\"]):\n",
        "            df[\"unemployment_inflation_ratio\"] = df[\"unemployment\"] / np.maximum(df[\"inflation\"], 0.1)\n",
        "\n",
        "        # Property-specific ratios and metrics\n",
        "        if all(col in df.columns for col in [\"noi\", \"value\"]):\n",
        "            df[\"noi_yield\"] = df[\"noi\"] / np.maximum(df[\"value\"], 1e-6)\n",
        "\n",
        "        if all(col in df.columns for col in [\"effective_rent\", \"sqft\"]):\n",
        "            df[\"rent_per_sqft_eff\"] = df[\"effective_rent\"] / np.maximum(df[\"sqft\"], 1)\n",
        "\n",
        "        if all(col in df.columns for col in [\"annual_debt_service\", \"noi\"]):\n",
        "            df[\"debt_service_coverage\"] = df[\"noi\"] / np.maximum(df[\"annual_debt_service\"], 1e-6)\n",
        "\n",
        "        # Market timing features\n",
        "        if \"age\" in df.columns:\n",
        "            df[\"age_squared\"] = df[\"age\"] ** 2\n",
        "            df[\"log_age\"] = np.log1p(df[\"age\"])\n",
        "\n",
        "        # Leverage metrics\n",
        "        if all(col in df.columns for col in [\"loan_amt\", \"value\"]):\n",
        "            df[\"ltv_calculated\"] = df[\"loan_amt\"] / np.maximum(df[\"value\"], 1e-6)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_market_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create market-level aggregated features\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        try:\n",
        "            # Market-level statistics by date\n",
        "            market_agg = df.groupby([\"date\", \"market\"]).agg({\n",
        "                \"total_return\": [\"mean\", \"std\", \"count\"],\n",
        "                \"cap_rate\": [\"mean\", \"median\"],\n",
        "                \"vacancy_rate\": [\"mean\"],\n",
        "                \"rent_psf\": [\"mean\", \"std\"]\n",
        "            })\n",
        "\n",
        "            # Flatten column names\n",
        "            market_agg.columns = [\"_\".join(col).strip() for col in market_agg.columns.values]\n",
        "            market_agg = market_agg.reset_index()\n",
        "\n",
        "            # Rename for clarity\n",
        "            rename_dict = {\n",
        "                \"total_return_mean\": \"market_avg_return\",\n",
        "                \"total_return_std\": \"market_return_volatility\",\n",
        "                \"total_return_count\": \"market_property_count\",\n",
        "                \"cap_rate_mean\": \"market_avg_cap_rate\",\n",
        "                \"cap_rate_median\": \"market_median_cap_rate\",\n",
        "                \"vacancy_rate_mean\": \"market_avg_vacancy\",\n",
        "                \"rent_psf_mean\": \"market_avg_rent_psf\",\n",
        "                \"rent_psf_std\": \"market_rent_volatility\"\n",
        "            }\n",
        "\n",
        "            market_agg = market_agg.rename(columns=rename_dict)\n",
        "\n",
        "            # Fill NaN values in volatility measures\n",
        "            market_agg[\"market_return_volatility\"] = market_agg[\"market_return_volatility\"].fillna(0)\n",
        "            market_agg[\"market_rent_volatility\"] = market_agg[\"market_rent_volatility\"].fillna(0)\n",
        "\n",
        "            # Merge back to main dataframe\n",
        "            df = df.merge(market_agg, on=[\"date\", \"market\"], how=\"left\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Error creating market features: {str(e)}\")\n",
        "            # Skip market features if they fail\n",
        "            pass\n",
        "\n",
        "        return df\n",
        "\n",
        "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Apply all feature engineering steps\"\"\"\n",
        "        self.logger.info(\"Starting feature engineering...\")\n",
        "        original_cols = len(df.columns)\n",
        "\n",
        "        # Apply feature engineering steps\n",
        "        df = self.create_time_features(df)\n",
        "\n",
        "        if self.config.enable_advanced_features:\n",
        "            df = self.create_lag_features(df)\n",
        "            df = self.create_rolling_features(df)\n",
        "            df = self.create_interaction_features(df)\n",
        "            df = self.create_market_features(df)\n",
        "\n",
        "        # Remove rows with NaN values created by lagging\n",
        "        df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "        new_cols = len(df.columns)\n",
        "        self.logger.info(f\"Feature engineering complete. Added {new_cols - original_cols} features. \"\n",
        "                        f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "# Advanced Model Framework\n",
        "\n",
        "class ModelFactory:\n",
        "    \"\"\"Factory for creating different types of models\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def create_preprocessor(numeric_cols: List[str], categorical_cols: List[str]) -> ColumnTransformer:\n",
        "        \"\"\"Create preprocessing pipeline\"\"\"\n",
        "        try:\n",
        "            # Try newer scikit-learn API\n",
        "            ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "        except TypeError:\n",
        "            # Fallback for older versions\n",
        "            ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "        preprocessor = ColumnTransformer([\n",
        "            (\"num\", RobustScaler(), numeric_cols),  # More robust to outliers\n",
        "            (\"cat\", ohe, categorical_cols)\n",
        "        ])\n",
        "\n",
        "        return preprocessor\n",
        "\n",
        "    @staticmethod\n",
        "    def create_model(model_type: str, random_state: int = 42) -> Any:\n",
        "        \"\"\"Create model based on type\"\"\"\n",
        "        models = {\n",
        "            'gradient_boosting': GradientBoostingRegressor(\n",
        "                random_state=random_state,\n",
        "                learning_rate=0.05,\n",
        "                n_estimators=500,\n",
        "                max_depth=4,\n",
        "                subsample=0.8,\n",
        "                validation_fraction=0.1,\n",
        "                n_iter_no_change=50\n",
        "            ),\n",
        "            'random_forest': RandomForestRegressor(\n",
        "                random_state=random_state,\n",
        "                n_estimators=300,\n",
        "                max_depth=8,\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=4,\n",
        "                bootstrap=True,\n",
        "                oob_score=True\n",
        "            ),\n",
        "            'ridge': Ridge(alpha=1.0),\n",
        "            'elastic_net': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=random_state)\n",
        "        }\n",
        "\n",
        "        return models.get(model_type, models['gradient_boosting'])\n",
        "\n",
        "    @staticmethod\n",
        "    def get_hyperparameter_grid(model_type: str) -> Dict[str, List]:\n",
        "        \"\"\"Get hyperparameter grid for model tuning\"\"\"\n",
        "        grids = {\n",
        "            'gradient_boosting': {\n",
        "                'model__learning_rate': [0.01, 0.05, 0.1],\n",
        "                'model__max_depth': [3, 4, 6],\n",
        "                'model__n_estimators': [300, 500],\n",
        "                'model__subsample': [0.8, 0.9]\n",
        "            },\n",
        "            'random_forest': {\n",
        "                'model__n_estimators': [200, 300],\n",
        "                'model__max_depth': [6, 8, 10],\n",
        "                'model__min_samples_split': [5, 10],\n",
        "                'model__min_samples_leaf': [2, 4]\n",
        "            },\n",
        "            'ridge': {\n",
        "                'model__alpha': [0.1, 1.0, 10.0, 100.0]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return grids.get(model_type, {})\n",
        "\n",
        "\n",
        "class AdvancedROIEngine:\n",
        "    \"\"\"Main ROI prediction engine with advanced capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, config: ROIEngineConfig, logger: logging.Logger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.feature_engineer = FeatureEngineer(config, logger)\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def load_or_generate_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load existing data or generate new synthetic data\"\"\"\n",
        "        if self.config.data_path.exists():\n",
        "            self.logger.info(f\"Loading existing data from {self.config.data_path}\")\n",
        "            df = pd.read_csv(self.config.data_path)\n",
        "            df[self.config.date_col] = pd.to_datetime(df[self.config.date_col])\n",
        "        else:\n",
        "            self.logger.info(\"Generating new synthetic dataset\")\n",
        "            generator = SyntheticDataGenerator(self.config, self.logger)\n",
        "            df = generator.generate_dataset(n_periods=120)\n",
        "            df.to_csv(self.config.data_path, index=False)\n",
        "            self.logger.info(f\"Saved synthetic data to {self.config.data_path}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
        "        \"\"\"Prepare data with feature engineering\"\"\"\n",
        "        # Apply feature engineering\n",
        "        df_processed = self.feature_engineer.engineer_features(df)\n",
        "\n",
        "        # Identify feature columns\n",
        "        exclude_cols = [self.config.target, self.config.date_col, \"value_next\", \"year_month\"]\n",
        "\n",
        "        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        numeric_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
        "\n",
        "        categorical_cols = [c for c in self.config.id_cols if c in df_processed.columns]\n",
        "        for c in df_processed.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
        "            if c not in [self.config.target, self.config.date_col] and c not in categorical_cols:\n",
        "                categorical_cols.append(c)\n",
        "\n",
        "        self.logger.info(f\"Prepared {len(numeric_cols)} numeric and {len(categorical_cols)} categorical features\")\n",
        "\n",
        "        return df_processed, numeric_cols, categorical_cols\n",
        "\n",
        "    def train_single_model(self, X: pd.DataFrame, y: np.ndarray,\n",
        "                          model_type: str, numeric_cols: List[str],\n",
        "                          categorical_cols: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"Train a single model with cross-validation\"\"\"\n",
        "\n",
        "        # Create preprocessing and model pipeline\n",
        "        preprocessor = ModelFactory.create_preprocessor(numeric_cols, categorical_cols)\n",
        "        model = ModelFactory.create_model(model_type, self.config.random_state)\n",
        "        pipeline = Pipeline([(\"preprocess\", preprocessor), (\"model\", model)])\n",
        "\n",
        "        # Hyperparameter tuning if enabled\n",
        "        if self.config.enable_hyperparameter_tuning:\n",
        "            param_grid = ModelFactory.get_hyperparameter_grid(model_type)\n",
        "            if param_grid:\n",
        "                self.logger.info(f\"Performing hyperparameter tuning for {model_type}\")\n",
        "\n",
        "                # Use fewer splits for tuning to speed up\n",
        "                tscv_tune = TimeSeriesSplit(n_splits=3)\n",
        "                grid_search = GridSearchCV(\n",
        "                    pipeline, param_grid, cv=tscv_tune,\n",
        "                    scoring='neg_mean_squared_error',\n",
        "                    n_jobs=self.config.n_jobs, verbose=1\n",
        "                )\n",
        "                grid_search.fit(X, y)\n",
        "                pipeline = grid_search.best_estimator_\n",
        "                self.logger.info(f\"Best parameters for {model_type}: {grid_search.best_params_}\")\n",
        "\n",
        "        # Cross-validation evaluation\n",
        "        tscv = TimeSeriesSplit(n_splits=self.config.n_splits)\n",
        "        cv_scores = {'mae': [], 'rmse': [], 'r2': [], 'mape': []}\n",
        "        y_true_all, y_pred_all = [], []\n",
        "\n",
        "        for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
        "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "            # Fit pipeline\n",
        "            pipeline.fit(X_train, y_train)\n",
        "            y_pred = pipeline.predict(X_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            mape = np.mean(np.abs((y_test - y_pred) / np.clip(np.abs(y_test), 1e-6, None))) * 100\n",
        "\n",
        "            cv_scores['mae'].append(mae)\n",
        "            cv_scores['rmse'].append(rmse)\n",
        "            cv_scores['r2'].append(r2)\n",
        "            cv_scores['mape'].append(mape)\n",
        "\n",
        "            y_true_all.extend(y_test.tolist())\n",
        "            y_pred_all.extend(y_pred.tolist())\n",
        "\n",
        "            self.logger.info(f\"{model_type} Fold {fold_idx}: MAE={mae:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
        "\n",
        "        # Fit final model on all data\n",
        "        pipeline.fit(X, y)\n",
        "\n",
        "        # Calculate average metrics\n",
        "        avg_metrics = {metric: np.mean(scores) for metric, scores in cv_scores.items()}\n",
        "        std_metrics = {metric: np.std(scores) for metric, scores in cv_scores.items()}\n",
        "\n",
        "        return {\n",
        "            'pipeline': pipeline,\n",
        "            'cv_metrics': cv_scores,\n",
        "            'avg_metrics': avg_metrics,\n",
        "            'std_metrics': std_metrics,\n",
        "            'y_true': y_true_all,\n",
        "            'y_pred': y_pred_all,\n",
        "            'model_type': model_type\n",
        "        }\n",
        "\n",
        "    def train_models(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Train multiple models and compare performance\"\"\"\n",
        "        self.logger.info(\"Starting model training...\")\n",
        "\n",
        "        # Prepare data\n",
        "        df_processed, numeric_cols, categorical_cols = self.prepare_data(df)\n",
        "\n",
        "        # Sort by date for proper time series splitting\n",
        "        df_processed = df_processed.sort_values(self.config.date_col).reset_index(drop=True)\n",
        "\n",
        "        X = df_processed.drop(columns=[self.config.target])\n",
        "        y = df_processed[self.config.target].values\n",
        "\n",
        "        self.logger.info(f\"Training dataset shape: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "        # Train each model\n",
        "        results = {}\n",
        "        for model_type in self.config.models_to_try:\n",
        "            self.logger.info(f\"Training {model_type} model...\")\n",
        "\n",
        "            try:\n",
        "                model_results = self.train_single_model(\n",
        "                    X, y, model_type, numeric_cols, categorical_cols\n",
        "                )\n",
        "                results[model_type] = model_results\n",
        "\n",
        "                avg_mae = model_results['avg_metrics']['mae']\n",
        "                avg_r2 = model_results['avg_metrics']['r2']\n",
        "                self.logger.info(f\"{model_type} completed - Average MAE: {avg_mae:.4f}, R²: {avg_r2:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error training {model_type}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Select best model based on R²\n",
        "        if results:\n",
        "            best_model = max(results.keys(), key=lambda k: results[k]['avg_metrics']['r2'])\n",
        "            self.logger.info(f\"Best model: {best_model} (R² = {results[best_model]['avg_metrics']['r2']:.4f})\")\n",
        "\n",
        "            # Store best model\n",
        "            self.best_model_name = best_model\n",
        "            self.best_model = results[best_model]['pipeline']\n",
        "\n",
        "        self.results = results\n",
        "        return results\n",
        "\n",
        "    def create_visualizations(self):\n",
        "        \"\"\"Create comprehensive visualization suite\"\"\"\n",
        "        if not self.results:\n",
        "            self.logger.warning(\"No results available for visualization\")\n",
        "            return\n",
        "\n",
        "        self.logger.info(\"Creating visualizations...\")\n",
        "\n",
        "        # Set up plotting style\n",
        "        plt.style.use('seaborn-v0_8')\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "\n",
        "        # 1. Model Comparison\n",
        "        self._plot_model_comparison()\n",
        "\n",
        "        # 2. Feature Importance (for best model)\n",
        "        if hasattr(self, 'best_model'):\n",
        "            self._plot_feature_importance()\n",
        "\n",
        "        # 3. Residual Analysis\n",
        "        self._plot_residual_analysis()\n",
        "\n",
        "        # 4. Prediction vs Actual\n",
        "        self._plot_predictions()\n",
        "\n",
        "        # 5. Time Series Analysis\n",
        "        self._plot_time_series_performance()\n",
        "\n",
        "        self.logger.info(\"Visualizations completed\")\n",
        "\n",
        "    def _plot_model_comparison(self):\n",
        "        \"\"\"Plot model performance comparison\"\"\"\n",
        "        if len(self.results) < 2:\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "        models = list(self.results.keys())\n",
        "        metrics = ['mae', 'rmse', 'r2', 'mape']\n",
        "        metric_names = ['Mean Absolute Error', 'Root Mean Squared Error', 'R² Score', 'Mean Absolute Percentage Error']\n",
        "\n",
        "        for idx, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "            ax = axes[idx//2, idx%2]\n",
        "\n",
        "            means = [self.results[model]['avg_metrics'][metric] for model in models]\n",
        "            stds = [self.results[model]['std_metrics'][metric] for model in models]\n",
        "\n",
        "            bars = ax.bar(models, means, yerr=stds, capsize=5, alpha=0.8)\n",
        "            ax.set_title(name, fontweight='bold')\n",
        "            ax.set_ylabel(name)\n",
        "\n",
        "            # Rotate x-axis labels if needed\n",
        "            if len(max(models, key=len)) > 8:\n",
        "                ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for bar, mean in zip(bars, means):\n",
        "                height = bar.get_height()\n",
        "                ax.annotate(f'{mean:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                           xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.config.plots_dir / f'model_comparison.{self.config.plot_format}',\n",
        "                   dpi=self.config.plot_dpi, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_feature_importance(self):\n",
        "        \"\"\"Plot feature importance for the best model\"\"\"\n",
        "        if not hasattr(self, 'best_model'):\n",
        "            return\n",
        "\n",
        "        # Extract model from pipeline\n",
        "        model = self.best_model.named_steps['model']\n",
        "        preprocessor = self.best_model.named_steps['preprocess']\n",
        "\n",
        "        if not hasattr(model, 'feature_importances_'):\n",
        "            return\n",
        "\n",
        "        # Get feature names\n",
        "        numeric_features = preprocessor.transformers_[0][2]\n",
        "        cat_encoder = preprocessor.transformers_[1][1]\n",
        "        cat_features = preprocessor.transformers_[1][2]\n",
        "\n",
        "        if hasattr(cat_encoder, 'get_feature_names_out'):\n",
        "            cat_feature_names = cat_encoder.get_feature_names_out(cat_features).tolist()\n",
        "        else:\n",
        "            cat_feature_names = []\n",
        "\n",
        "        feature_names = list(numeric_features) + cat_feature_names\n",
        "        importances = model.feature_importances_\n",
        "\n",
        "        # Sort by importance\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "        top_n = min(20, len(indices))\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.bar(range(top_n), importances[indices[:top_n]], alpha=0.8)\n",
        "        plt.title(f'Top {top_n} Feature Importances - {self.best_model_name.title()}',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Features')\n",
        "        plt.ylabel('Importance')\n",
        "\n",
        "        feature_labels = [feature_names[i] if i < len(feature_names) else f'Feature_{i}'\n",
        "                         for i in indices[:top_n]]\n",
        "        plt.xticks(range(top_n), feature_labels, rotation=90)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.config.plots_dir / f'feature_importance.{self.config.plot_format}',\n",
        "                   dpi=self.config.plot_dpi, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_residual_analysis(self):\n",
        "        \"\"\"Create residual analysis plots\"\"\"\n",
        "        if not hasattr(self, 'best_model_name'):\n",
        "            return\n",
        "\n",
        "        best_results = self.results[self.best_model_name]\n",
        "        y_true = np.array(best_results['y_true'])\n",
        "        y_pred = np.array(best_results['y_pred'])\n",
        "        residuals = y_true - y_pred\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle(f'Residual Analysis - {self.best_model_name.title()}',\n",
        "                    fontsize=16, fontweight='bold')\n",
        "\n",
        "        # 1. Residuals vs Predicted\n",
        "        axes[0,0].scatter(y_pred, residuals, alpha=0.6, s=20)\n",
        "        axes[0,0].axhline(y=0, color='red', linestyle='--')\n",
        "        axes[0,0].set_xlabel('Predicted Values')\n",
        "        axes[0,0].set_ylabel('Residuals')\n",
        "        axes[0,0].set_title('Residuals vs Predicted')\n",
        "\n",
        "        # 2. Q-Q Plot\n",
        "        from scipy import stats\n",
        "        stats.probplot(residuals, dist=\"norm\", plot=axes[0,1])\n",
        "        axes[0,1].set_title('Q-Q Plot (Normal Distribution)')\n",
        "\n",
        "        # 3. Histogram of Residuals\n",
        "        axes[1,0].hist(residuals, bins=30, alpha=0.7, density=True)\n",
        "        axes[1,0].set_xlabel('Residuals')\n",
        "        axes[1,0].set_ylabel('Density')\n",
        "        axes[1,0].set_title('Distribution of Residuals')\n",
        "\n",
        "        # Add normal curve overlay\n",
        "        mu, sigma = stats.norm.fit(residuals)\n",
        "        x = np.linspace(residuals.min(), residuals.max(), 100)\n",
        "        axes[1,0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', lw=2, label='Normal Fit')\n",
        "        axes[1,0].legend()\n",
        "\n",
        "        # 4. Scale-Location Plot\n",
        "        sqrt_abs_resid = np.sqrt(np.abs(residuals))\n",
        "        axes[1,1].scatter(y_pred, sqrt_abs_resid, alpha=0.6, s=20)\n",
        "        axes[1,1].set_xlabel('Predicted Values')\n",
        "        axes[1,1].set_ylabel('√|Residuals|')\n",
        "        axes[1,1].set_title('Scale-Location Plot')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.config.plots_dir / f'residual_analysis.{self.config.plot_format}',\n",
        "                   dpi=self.config.plot_dpi, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_predictions(self):\n",
        "        \"\"\"Plot predictions vs actual values\"\"\"\n",
        "        if not hasattr(self, 'best_model_name'):\n",
        "            return\n",
        "\n",
        "        best_results = self.results[self.best_model_name]\n",
        "        y_true = np.array(best_results['y_true'])\n",
        "        y_pred = np.array(best_results['y_pred'])\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        # Scatter plot\n",
        "        plt.scatter(y_true, y_pred, alpha=0.6, s=20)\n",
        "\n",
        "        # Perfect prediction line\n",
        "        min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "\n",
        "        # Calculate R²\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        plt.text(0.05, 0.95, f'R² = {r2:.4f}', transform=plt.gca().transAxes,\n",
        "                fontsize=12, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.xlabel('Actual Values')\n",
        "        plt.ylabel('Predicted Values')\n",
        "        plt.title(f'Predictions vs Actual - {self.best_model_name.title()}',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.config.plots_dir / f'predictions_vs_actual.{self.config.plot_format}',\n",
        "                   dpi=self.config.plot_dpi, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_time_series_performance(self):\n",
        "        \"\"\"Plot model performance over time\"\"\"\n",
        "        # This would require additional date tracking in cross-validation\n",
        "        # For now, we'll create a placeholder\n",
        "        pass\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save all results and models\"\"\"\n",
        "        self.logger.info(\"Saving results...\")\n",
        "\n",
        "        # Save model comparison metrics\n",
        "        comparison_data = []\n",
        "        for model_name, results in self.results.items():\n",
        "            row = {'model': model_name}\n",
        "            row.update(results['avg_metrics'])\n",
        "            row.update({f'{k}_std': v for k, v in results['std_metrics'].items()})\n",
        "            comparison_data.append(row)\n",
        "\n",
        "        comparison_df = pd.DataFrame(comparison_data)\n",
        "        comparison_df.to_csv(self.config.artifacts_dir / 'model_comparison.csv', index=False)\n",
        "\n",
        "        # Save detailed results\n",
        "        with open(self.config.artifacts_dir / 'detailed_results.json', 'w') as f:\n",
        "            # Convert results to JSON-serializable format\n",
        "            json_results = {}\n",
        "            for model_name, results in self.results.items():\n",
        "                json_results[model_name] = {\n",
        "                    'cv_metrics': results['cv_metrics'],\n",
        "                    'avg_metrics': results['avg_metrics'],\n",
        "                    'std_metrics': results['std_metrics'],\n",
        "                    'model_type': results['model_type']\n",
        "                    # Note: pipeline and predictions are not JSON serializable\n",
        "                }\n",
        "            json.dump(json_results, f, indent=2)\n",
        "\n",
        "        # Save best model\n",
        "        if hasattr(self, 'best_model'):\n",
        "            joblib.dump(self.best_model, self.config.artifacts_dir / 'best_model.joblib')\n",
        "\n",
        "            # Save predictions from best model\n",
        "            best_results = self.results[self.best_model_name]\n",
        "            predictions_df = pd.DataFrame({\n",
        "                'y_true': best_results['y_true'],\n",
        "                'y_pred': best_results['y_pred'],\n",
        "                'residuals': np.array(best_results['y_true']) - np.array(best_results['y_pred'])\n",
        "            })\n",
        "            predictions_df.to_csv(self.config.artifacts_dir / 'best_model_predictions.csv', index=False)\n",
        "\n",
        "        self.logger.info(f\"Results saved to {self.config.artifacts_dir}\")\n",
        "\n",
        "    def run_complete_analysis(self) -> Dict[str, Any]:\n",
        "        \"\"\"Run the complete ROI analysis pipeline\"\"\"\n",
        "        self.logger.info(\"Starting complete ROI analysis...\")\n",
        "\n",
        "        try:\n",
        "            # Load or generate data\n",
        "            df = self.load_or_generate_data()\n",
        "\n",
        "            # Train models\n",
        "            results = self.train_models(df)\n",
        "\n",
        "            # Create visualizations\n",
        "            if self.config.save_plots:\n",
        "                self.create_visualizations()\n",
        "\n",
        "            # Save results\n",
        "            self.save_results()\n",
        "\n",
        "            # Print summary\n",
        "            self._print_summary()\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in analysis pipeline: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _print_summary(self):\n",
        "        \"\"\"Print analysis summary\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ROI ENGINE ANALYSIS SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        if not self.results:\n",
        "            print(\"No results available.\")\n",
        "            return\n",
        "\n",
        "        # Model comparison\n",
        "        print(\"\\nModel Performance Comparison:\")\n",
        "        print(\"-\" * 40)\n",
        "        for model_name, results in self.results.items():\n",
        "            metrics = results['avg_metrics']\n",
        "            print(f\"{model_name.upper():<20} | R²: {metrics['r2']:.4f} | MAE: {metrics['mae']:.4f} | RMSE: {metrics['rmse']:.4f}\")\n",
        "\n",
        "        # Best model details\n",
        "        if hasattr(self, 'best_model_name'):\n",
        "            print(f\"\\nBest Model: {self.best_model_name.upper()}\")\n",
        "            best_metrics = self.results[self.best_model_name]['avg_metrics']\n",
        "            print(f\"  R² Score: {best_metrics['r2']:.4f}\")\n",
        "            print(f\"  Mean Absolute Error: {best_metrics['mae']:.4f}\")\n",
        "            print(f\"  Root Mean Squared Error: {best_metrics['rmse']:.4f}\")\n",
        "            print(f\"  Mean Absolute Percentage Error: {best_metrics['mape']:.2f}%\")\n",
        "\n",
        "        print(f\"\\nResults saved to: {self.config.artifacts_dir}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5) Main Execution\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Initialize configuration\n",
        "    config = ROIEngineConfig(\n",
        "        enable_advanced_features=True,\n",
        "        enable_hyperparameter_tuning=True,\n",
        "        models_to_try=['gradient_boosting', 'random_forest', 'ridge'],\n",
        "        save_plots=True\n",
        "    )\n",
        "\n",
        "    # Setup logging\n",
        "    logger = setup_logging(config)\n",
        "    logger.info(\"Starting Advanced ROI Engine\")\n",
        "\n",
        "    try:\n",
        "        # Initialize and run engine\n",
        "        engine = AdvancedROIEngine(config, logger)\n",
        "        results = engine.run_complete_analysis()\n",
        "\n",
        "        logger.info(\"Analysis completed successfully\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Analysis failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQW9bu9yZJSK",
        "outputId": "93a953b7-1bba-4f76-b37e-339218816f6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ROI_Engine:Error training gradient_boosting: got an unexpected keyword argument 'squared'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ROI_Engine:Error training random_forest: got an unexpected keyword argument 'squared'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ROI_Engine:Error training ridge: got an unexpected keyword argument 'squared'\n",
            "WARNING:ROI_Engine:No results available for visualization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ROI ENGINE ANALYSIS SUMMARY\n",
            "============================================================\n",
            "No results available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Advanced ROI Engine for Real Estate Investments (CRE)\n",
        "# Enhanced version with robust error handling, feature engineering, and model optimization\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any, Optional, Union\n",
        "from dataclasses import dataclass, field\n",
        "import logging\n",
        "\n",
        "# ML imports\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, QuantileTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import joblib\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# =========================\n",
        "# 0) Configuration & Setup\n",
        "# =========================\n",
        "\n",
        "@dataclass\n",
        "class ROIEngineConfig:\n",
        "    \"\"\"Advanced configuration for ROI Engine\"\"\"\n",
        "    # Data paths\n",
        "    base_dir: str = \"./roi_engine_project\"\n",
        "    data_filename: str = \"synthetic_cre_dataset.csv\"\n",
        "\n",
        "    # Model settings\n",
        "    target: str = \"total_return\"\n",
        "    date_col: str = \"date\"\n",
        "    id_cols: List[str] = field(default_factory=lambda: [\"market\", \"asset_type\"])\n",
        "\n",
        "    # Cross-validation\n",
        "    n_splits: int = 5\n",
        "    test_size_ratio: float = 0.2\n",
        "\n",
        "    # Model parameters\n",
        "    random_state: int = 42\n",
        "    n_jobs: int = -1\n",
        "\n",
        "    # Feature engineering\n",
        "    enable_advanced_features: bool = True\n",
        "    rolling_windows: List[int] = field(default_factory=lambda: [3, 6, 12])\n",
        "    lag_periods: List[int] = field(default_factory=lambda: [1, 2, 3, 6])\n",
        "\n",
        "    # Model selection\n",
        "    models_to_try: List[str] = field(default_factory=lambda: ['gradient_boosting', 'random_forest', 'ridge'])\n",
        "    enable_hyperparameter_tuning: bool = True\n",
        "\n",
        "    # Output settings\n",
        "    save_plots: bool = True\n",
        "    plot_format: str = \"png\"\n",
        "    plot_dpi: int = 300\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.data_dir = Path(self.base_dir) / \"data\"\n",
        "        self.artifacts_dir = Path(self.base_dir) / \"artifacts\"\n",
        "        self.plots_dir = self.artifacts_dir / \"plots\"\n",
        "\n",
        "        # Create directories\n",
        "        for dir_path in [self.data_dir, self.artifacts_dir, self.plots_dir]:\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.data_path = self.data_dir / self.data_filename\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1) Logging Setup\n",
        "# =========================\n",
        "\n",
        "def setup_logging(config: ROIEngineConfig) -> logging.Logger:\n",
        "    \"\"\"Setup logging configuration\"\"\"\n",
        "    log_file = config.artifacts_dir / \"roi_engine.log\"\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    logger = logging.getLogger('ROI_Engine')\n",
        "    return logger\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2) Enhanced Data Generation\n",
        "# =========================\n",
        "\n",
        "class SyntheticDataGenerator:\n",
        "    \"\"\"Enhanced synthetic CRE data generator with realistic market dynamics\"\"\"\n",
        "\n",
        "    def __init__(self, config: ROIEngineConfig, logger: logging.Logger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "    def generate_macroeconomic_data(self, n_periods: int, start_date: str = \"2016-01-01\") -> pd.DataFrame:\n",
        "        \"\"\"Generate realistic macroeconomic time series\"\"\"\n",
        "        np.random.seed(self.config.random_state)\n",
        "\n",
        "        dates = pd.date_range(start_date, periods=n_periods, freq=\"M\")\n",
        "        time_trend = np.linspace(0, 8, n_periods)\n",
        "\n",
        "        # More realistic macro indicators with autocorrelation\n",
        "        inflation_base = 2.0 + 0.8 * np.sin(time_trend) + np.random.normal(0, 0.3, n_periods)\n",
        "        inflation = np.cumsum(np.random.normal(0, 0.1, n_periods)) + inflation_base\n",
        "        inflation = np.clip(inflation, 0.5, 6.0)\n",
        "\n",
        "        fed_rate_changes = np.random.normal(0, 0.05, n_periods)\n",
        "        fed_rate_changes[::24] += np.random.normal(0, 0.3, len(fed_rate_changes[::24]))  # Policy changes\n",
        "        fed_rate = np.clip(np.cumsum(fed_rate_changes) + 1.5, 0.1, 8.0)\n",
        "\n",
        "        gdp_base = 2.5 + 0.5 * np.cos(time_trend * 1.2)\n",
        "        gdp_shocks = np.random.choice([0, 0, 0, 0, -2, 1.5], n_periods, p=[0.7, 0.1, 0.1, 0.05, 0.03, 0.02])\n",
        "        gdp_growth = gdp_base + gdp_shocks + np.random.normal(0, 0.2, n_periods)\n",
        "        gdp_growth = np.clip(gdp_growth, -3.0, 6.0)\n",
        "\n",
        "        # Employment and credit conditions\n",
        "        unemployment = np.clip(6.0 - 0.3 * gdp_growth + np.random.normal(0, 0.3, n_periods), 3.0, 12.0)\n",
        "        credit_spread = np.clip(1.5 + 0.2 * (fed_rate - 2.0) - 0.1 * gdp_growth +\n",
        "                               np.random.normal(0, 0.2, n_periods), 0.5, 4.0)\n",
        "\n",
        "        return pd.DataFrame({\n",
        "            'date': dates,\n",
        "            'inflation': inflation,\n",
        "            'fed_rate': fed_rate,\n",
        "            'gdp_growth': gdp_growth,\n",
        "            'unemployment': unemployment,\n",
        "            'credit_spread': credit_spread\n",
        "        })\n",
        "\n",
        "    def generate_market_data(self, macro_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Generate market-specific real estate data\"\"\"\n",
        "        n_periods = len(macro_df)\n",
        "\n",
        "        # Market definitions with more granular characteristics\n",
        "        market_params = {\n",
        "            \"NYC\": {\"base_rent\": 85, \"volatility\": 0.15, \"liquidity\": 0.9, \"barrier_to_entry\": 0.8},\n",
        "            \"LA\": {\"base_rent\": 55, \"volatility\": 0.18, \"liquidity\": 0.7, \"barrier_to_entry\": 0.6},\n",
        "            \"SF\": {\"base_rent\": 75, \"volatility\": 0.22, \"liquidity\": 0.6, \"barrier_to_entry\": 0.9},\n",
        "            \"CHI\": {\"base_rent\": 40, \"volatility\": 0.12, \"liquidity\": 0.8, \"barrier_to_entry\": 0.4},\n",
        "            \"DAL\": {\"base_rent\": 35, \"volatility\": 0.16, \"liquidity\": 0.7, \"barrier_to_entry\": 0.3},\n",
        "            \"BOS\": {\"base_rent\": 65, \"volatility\": 0.14, \"liquidity\": 0.6, \"barrier_to_entry\": 0.7},\n",
        "            \"SEA\": {\"base_rent\": 60, \"volatility\": 0.20, \"liquidity\": 0.5, \"barrier_to_entry\": 0.6}\n",
        "        }\n",
        "\n",
        "        asset_type_params = {\n",
        "            \"Multifamily\": {\"alpha\": 1.15, \"beta_gdp\": 0.3, \"beta_unemployment\": -0.4, \"cyclicality\": 0.7},\n",
        "            \"Office\": {\"alpha\": 1.0, \"beta_gdp\": 0.5, \"beta_unemployment\": -0.6, \"cyclicality\": 1.2},\n",
        "            \"Industrial\": {\"alpha\": 1.05, \"beta_gdp\": 0.4, \"beta_unemployment\": -0.2, \"cyclicality\": 0.8},\n",
        "            \"Retail\": {\"alpha\": 0.95, \"beta_gdp\": 0.6, \"beta_unemployment\": -0.8, \"cyclicality\": 1.5},\n",
        "            \"Hotel\": {\"alpha\": 1.3, \"beta_gdp\": 0.8, \"beta_unemployment\": -1.0, \"cyclicality\": 2.0}\n",
        "        }\n",
        "\n",
        "        records = []\n",
        "\n",
        "        for _, row in macro_df.iterrows():\n",
        "            # Generate multiple properties per period for more data\n",
        "            n_properties = np.random.randint(8, 15)\n",
        "\n",
        "            for _ in range(n_properties):\n",
        "                market = np.random.choice(list(market_params.keys()))\n",
        "                asset_type = np.random.choice(list(asset_type_params.keys()))\n",
        "\n",
        "                mp = market_params[market]\n",
        "                atp = asset_type_params[asset_type]\n",
        "\n",
        "                # Property characteristics\n",
        "                sqft = np.random.randint(25_000, 500_000)\n",
        "                age = np.random.randint(1, 50)\n",
        "                quality_score = np.random.uniform(0.6, 1.0)\n",
        "\n",
        "                # Dynamic rent calculation\n",
        "                base_rent = mp[\"base_rent\"] * atp[\"alpha\"] * quality_score\n",
        "\n",
        "                # Economic sensitivity\n",
        "                gdp_effect = atp[\"beta_gdp\"] * (row[\"gdp_growth\"] - 2.5) / 2.5\n",
        "                unemployment_effect = atp[\"beta_unemployment\"] * (row[\"unemployment\"] - 6.0) / 6.0\n",
        "\n",
        "                market_volatility = mp[\"volatility\"] * atp[\"cyclicality\"]\n",
        "                rent_shock = np.random.normal(0, market_volatility)\n",
        "\n",
        "                rent_psf = base_rent * (1 + gdp_effect + unemployment_effect + rent_shock)\n",
        "                rent_psf = np.clip(rent_psf, base_rent * 0.5, base_rent * 2.0)\n",
        "\n",
        "                annual_rent = rent_psf * sqft\n",
        "\n",
        "                # Vacancy dynamics\n",
        "                base_vacancy = 5.0 + np.random.uniform(-1, 3)\n",
        "                vacancy_sensitivity = -unemployment_effect * 2 + (row[\"fed_rate\"] - 2.0) * 0.5\n",
        "                vacancy_rate = np.clip(base_vacancy + vacancy_sensitivity +\n",
        "                                     np.random.normal(0, 1.5), 1.0, 25.0)\n",
        "\n",
        "                effective_rent = annual_rent * (1 - vacancy_rate / 100.0)\n",
        "\n",
        "                # Operating expenses with inflation sensitivity\n",
        "                opex_base = 0.25 + 0.05 * np.random.rand()\n",
        "                opex_inflation = 0.5 * (row[\"inflation\"] - 2.0) / 2.0\n",
        "                opex_rate = np.clip(opex_base + opex_inflation, 0.15, 0.45)\n",
        "                opex = effective_rent * opex_rate\n",
        "\n",
        "                noi = effective_rent - opex\n",
        "\n",
        "                # Cap rates with risk premiums\n",
        "                base_cap = 4.5 + 0.5 * (1 - mp[\"liquidity\"]) + 0.3 * atp[\"cyclicality\"]\n",
        "                risk_premium = 0.3 * (row[\"fed_rate\"] - 2.0) + 0.2 * row[\"credit_spread\"]\n",
        "                market_risk = -0.1 * (row[\"gdp_growth\"] - 2.5) / 2.5\n",
        "\n",
        "                cap_rate = np.clip(base_cap + risk_premium + market_risk +\n",
        "                                  np.random.normal(0, 0.2), 3.0, 12.0)\n",
        "\n",
        "                value = noi / (cap_rate / 100.0) if cap_rate > 0 else 0\n",
        "\n",
        "                # Financing\n",
        "                ltv_base = 0.65\n",
        "                ltv_adjustment = -0.05 * (row[\"fed_rate\"] - 2.0) / 2.0 - 0.03 * row[\"credit_spread\"]\n",
        "                ltv = np.clip(ltv_base + ltv_adjustment + np.random.normal(0, 0.05), 0.4, 0.8)\n",
        "\n",
        "                loan_amt = value * ltv\n",
        "                coupon = (row[\"fed_rate\"] + row[\"credit_spread\"] + 1.5 +\n",
        "                         0.5 * atp[\"cyclicality\"] + np.random.normal(0, 0.3)) / 100.0\n",
        "                coupon = np.clip(coupon, 0.02, 0.12)\n",
        "\n",
        "                annual_debt_service = loan_amt * coupon\n",
        "\n",
        "                # Returns calculation with market dynamics\n",
        "                market_appreciation = (0.02 + 0.5 * (row[\"gdp_growth\"] - 2.5) / 2.5 -\n",
        "                                     0.3 * (row[\"fed_rate\"] - 2.0) / 2.0)\n",
        "                asset_specific = atp[\"beta_gdp\"] * (row[\"gdp_growth\"] - 2.5) / 10.0\n",
        "                idiosyncratic = np.random.normal(0, market_volatility * 0.5)\n",
        "\n",
        "                appreciation_rate = market_appreciation + asset_specific + idiosyncratic\n",
        "                value_next = value * (1 + appreciation_rate)\n",
        "\n",
        "                equity = np.maximum(value - loan_amt, 1e-6)\n",
        "                cash_flow = noi - annual_debt_service\n",
        "                total_return = (cash_flow + (value_next - value)) / equity\n",
        "\n",
        "                # Additional metrics\n",
        "                dscr = noi / annual_debt_service if annual_debt_service > 0 else np.inf\n",
        "                cash_on_cash = cash_flow / equity\n",
        "\n",
        "                records.append({\n",
        "                    \"date\": row[\"date\"],\n",
        "                    \"market\": market,\n",
        "                    \"asset_type\": asset_type,\n",
        "                    \"sqft\": sqft,\n",
        "                    \"age\": age,\n",
        "                    \"quality_score\": quality_score,\n",
        "                    \"rent_psf\": rent_psf,\n",
        "                    \"annual_rent\": annual_rent,\n",
        "                    \"vacancy_rate\": vacancy_rate,\n",
        "                    \"effective_rent\": effective_rent,\n",
        "                    \"opex_rate\": opex_rate,\n",
        "                    \"opex\": opex,\n",
        "                    \"noi\": noi,\n",
        "                    \"cap_rate\": cap_rate,\n",
        "                    \"value\": value,\n",
        "                    \"ltv\": ltv,\n",
        "                    \"loan_amt\": loan_amt,\n",
        "                    \"coupon\": coupon * 100.0,\n",
        "                    \"annual_debt_service\": annual_debt_service,\n",
        "                    \"equity\": equity,\n",
        "                    \"value_next\": value_next,\n",
        "                    \"total_return\": total_return,\n",
        "                    \"dscr\": dscr,\n",
        "                    \"cash_on_cash\": cash_on_cash,\n",
        "                    **{k: row[k] for k in [\"inflation\", \"fed_rate\", \"gdp_growth\", \"unemployment\", \"credit_spread\"]}\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "    def generate_dataset(self, n_periods: int = 120) -> pd.DataFrame:\n",
        "        \"\"\"Generate complete synthetic dataset\"\"\"\n",
        "        self.logger.info(f\"Generating synthetic dataset with {n_periods} periods\")\n",
        "\n",
        "        # Generate macroeconomic data\n",
        "        macro_df = self.generate_macroeconomic_data(n_periods)\n",
        "\n",
        "        # Generate market data\n",
        "        market_df = self.generate_market_data(macro_df)\n",
        "\n",
        "        self.logger.info(f\"Generated {len(market_df)} property records\")\n",
        "        return market_df\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3) Advanced Feature Engineering\n",
        "# =========================\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Advanced feature engineering for real estate data\"\"\"\n",
        "\n",
        "    def __init__(self, config: ROIEngineConfig, logger: logging.Logger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "    def create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create comprehensive time-based features\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Basic time features\n",
        "        df[\"year\"] = df[self.config.date_col].dt.year\n",
        "        df[\"month\"] = df[self.config.date_col].dt.month\n",
        "        df[\"quarter\"] = df[self.config.date_col].dt.quarter\n",
        "        df[\"year_month\"] = df[self.config.date_col].dt.to_period('M').astype(str)\n",
        "\n",
        "        # Cyclical encoding for seasonal patterns\n",
        "        df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
        "        df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
        "        df[\"quarter_sin\"] = np.sin(2 * np.pi * df[\"quarter\"] / 4)\n",
        "        df[\"quarter_cos\"] = np.cos(2 * np.pi * df[\"quarter\"] / 4)\n",
        "\n",
        "        # Time since start\n",
        "        min_date = df[self.config.date_col].min()\n",
        "        df[\"months_since_start\"] = ((df[self.config.date_col] - min_date).dt.days / 30.44).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_lag_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create lagged features for key variables\"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values([self.config.date_col] + self.config.id_cols).reset_index(drop=True)\n",
        "\n",
        "        # Key variables to lag\n",
        "        lag_vars = [\n",
        "            \"noi\", \"value\", \"cap_rate\", \"vacancy_rate\", \"rent_psf\",\n",
        "            \"fed_rate\", \"inflation\", \"gdp_growth\", \"unemployment\", \"credit_spread\",\n",
        "            \"total_return\", \"effective_rent\", \"opex_rate\"\n",
        "        ]\n",
        "\n",
        "        # Create lags for each variable\n",
        "        for var in lag_vars:\n",
        "            if var in df.columns:\n",
        "                for lag in self.config.lag_periods:\n",
        "                    try:\n",
        "                        # Group by market and asset type for proper lagging\n",
        "                        lagged_values = df.groupby(self.config.id_cols, group_keys=False)[var].shift(lag)\n",
        "                        df[f\"{var}_lag{lag}\"] = lagged_values.values  # Extract values to avoid index issues\n",
        "                    except Exception as e:\n",
        "                        self.logger.warning(f\"Error creating lag feature {var}_lag{lag}: {str(e)}\")\n",
        "                        # Fallback: simple shift without grouping\n",
        "                        df[f\"{var}_lag{lag}\"] = df[var].shift(lag)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_rolling_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create rolling window statistics\"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values([self.config.date_col] + self.config.id_cols).reset_index(drop=True)\n",
        "\n",
        "        # Variables for rolling features\n",
        "        rolling_vars = [\n",
        "            \"total_return\", \"noi\", \"cap_rate\", \"vacancy_rate\", \"rent_psf\",\n",
        "            \"fed_rate\", \"gdp_growth\", \"inflation\"\n",
        "        ]\n",
        "\n",
        "        for var in rolling_vars:\n",
        "            if var in df.columns:\n",
        "                for window in self.config.rolling_windows:\n",
        "                    try:\n",
        "                        # Use transform to maintain proper alignment\n",
        "                        grouped = df.groupby(self.config.id_cols, group_keys=False)[var]\n",
        "\n",
        "                        # Rolling mean and std\n",
        "                        df[f\"{var}_roll_mean_{window}\"] = grouped.rolling(\n",
        "                            window=window, min_periods=1\n",
        "                        ).mean().reset_index(level=self.config.id_cols, drop=True)\n",
        "\n",
        "                        df[f\"{var}_roll_std_{window}\"] = grouped.rolling(\n",
        "                            window=window, min_periods=1\n",
        "                        ).std().reset_index(level=self.config.id_cols, drop=True)\n",
        "\n",
        "                        # Rolling min/max\n",
        "                        df[f\"{var}_roll_min_{window}\"] = grouped.rolling(\n",
        "                            window=window, min_periods=1\n",
        "                        ).min().reset_index(level=self.config.id_cols, drop=True)\n",
        "\n",
        "                        df[f\"{var}_roll_max_{window}\"] = grouped.rolling(\n",
        "                            window=window, min_periods=1\n",
        "                        ).max().reset_index(level=self.config.id_cols, drop=True)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        self.logger.warning(f\"Error creating rolling feature {var}_roll_*_{window}: {str(e)}\")\n",
        "                        # Fallback: simple rolling without grouping\n",
        "                        try:\n",
        "                            df[f\"{var}_roll_mean_{window}\"] = df[var].rolling(window=window, min_periods=1).mean()\n",
        "                            df[f\"{var}_roll_std_{window}\"] = df[var].rolling(window=window, min_periods=1).std()\n",
        "                            df[f\"{var}_roll_min_{window}\"] = df[var].rolling(window=window, min_periods=1).min()\n",
        "                            df[f\"{var}_roll_max_{window}\"] = df[var].rolling(window=window, min_periods=1).max()\n",
        "                        except Exception as e2:\n",
        "                            self.logger.warning(f\"Fallback rolling feature creation also failed for {var}: {str(e2)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create interaction and derived features\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Economic interaction terms\n",
        "        if all(col in df.columns for col in [\"gdp_growth\", \"fed_rate\"]):\n",
        "            df[\"gdp_fed_interaction\"] = df[\"gdp_growth\"] * df[\"fed_rate\"]\n",
        "\n",
        "        if all(col in df.columns for col in [\"unemployment\", \"inflation\"]):\n",
        "            df[\"unemployment_inflation_ratio\"] = df[\"unemployment\"] / np.maximum(df[\"inflation\"], 0.1)\n",
        "\n",
        "        # Property-specific ratios and metrics\n",
        "        if all(col in df.columns for col in [\"noi\", \"value\"]):\n",
        "            df[\"noi_yield\"] = df[\"noi\"] / np.maximum(df[\"value\"], 1e-6)\n",
        "\n",
        "        if all(col in df.columns for col in [\"effective_rent\", \"sqft\"]):\n",
        "            df[\"rent_per_sqft_eff\"] = df[\"effective_rent\"] / np.maximum(df[\"sqft\"], 1)\n",
        "\n",
        "        if all(col in df.columns for col in [\"annual_debt_service\", \"noi\"]):\n",
        "            df[\"debt_service_coverage\"] = df[\"noi\"] / np.maximum(df[\"annual_debt_service\"], 1e-6)\n",
        "\n",
        "        # Market timing features\n",
        "        if \"age\" in df.columns:\n",
        "            df[\"age_squared\"] = df[\"age\"] ** 2\n",
        "            df[\"log_age\"] = np.log1p(df[\"age\"])\n",
        "\n",
        "        # Leverage metrics\n",
        "        if all(col in df.columns for col in [\"loan_amt\", \"value\"]):\n",
        "            df[\"ltv_calculated\"] = df[\"loan_amt\"] / np.maximum(df[\"value\"], 1e-6)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_market_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create market-level aggregated features\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        try:\n",
        "            # Market-level statistics by date\n",
        "            market_agg = df.groupby([\"date\", \"market\"]).agg({\n",
        "                \"total_return\": [\"mean\", \"std\", \"count\"],\n",
        "                \"cap_rate\": [\"mean\", \"median\"],\n",
        "                \"vacancy_rate\": [\"mean\"],\n",
        "                \"rent_psf\": [\"mean\", \"std\"]\n",
        "            })\n",
        "\n",
        "            # Flatten column names\n",
        "            market_agg.columns = [\"_\".join(col).strip() for col in market_agg.columns.values]\n",
        "            market_agg = market_agg.reset_index()\n",
        "\n",
        "            # Rename for clarity\n",
        "            rename_dict = {\n",
        "                \"total_return_mean\": \"market_avg_return\",\n",
        "                \"total_return_std\": \"market_return_volatility\",\n",
        "                \"total_return_count\": \"market_property_count\",\n",
        "                \"cap_rate_mean\": \"market_avg_cap_rate\",\n",
        "                \"cap_rate_median\": \"market_median_cap_rate\",\n",
        "                \"vacancy_rate_mean\": \"market_avg_vacancy\",\n",
        "                \"rent_psf_mean\": \"market_avg_rent_psf\",\n",
        "                \"rent_psf_std\": \"market_rent_volatility\"\n",
        "            }\n",
        "\n",
        "            market_agg = market_agg.rename(columns=rename_dict)\n",
        "\n",
        "            # Fill NaN values in volatility measures\n",
        "            market_agg[\"market_return_volatility\"] = market_agg[\"market_return_volatility\"].fillna(0)\n",
        "            market_agg[\"market_rent_volatility\"] = market_agg[\"market_rent_volatility\"].fillna(0)\n",
        "\n",
        "            # Merge back to main dataframe\n",
        "            df = df.merge(market_agg, on=[\"date\", \"market\"], how=\"left\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Error creating market features: {str(e)}\")\n",
        "            # Skip market features if they fail\n",
        "            pass\n",
        "\n",
        "        return df\n",
        "\n",
        "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Apply all feature engineering steps\"\"\"\n",
        "        self.logger.info(\"Starting feature engineering...\")\n",
        "        original_cols = len(df.columns)\n",
        "\n",
        "        # Apply feature engineering steps\n",
        "        df = self.create_time_features(df)\n",
        "\n",
        "        if self.config.enable_advanced_features:\n",
        "            df = self.create_lag_features(df)\n",
        "            df = self.create_rolling_features(df)\n",
        "            df = self.create_interaction_features(df)\n",
        "            df = self.create_market_features(df)\n",
        "\n",
        "        # Remove rows with NaN values created by lagging\n",
        "        df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "        new_cols = len(df.columns)\n",
        "        self.logger.info(f\"Feature engineering complete. Added {new_cols - original_cols} features. \"\n",
        "                        f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4) Advanced Model Framework\n",
        "# =========================\n",
        "\n",
        "class ModelFactory:\n",
        "    \"\"\"Factory for creating different types of models\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def create_preprocessor(numeric_cols: List[str], categorical_cols: List[str]) -> ColumnTransformer:\n",
        "        \"\"\"Create preprocessing pipeline\"\"\"\n",
        "        try:\n",
        "            # Try newer scikit-learn API\n",
        "            ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "        except TypeError:\n",
        "            # Fallback for older versions\n",
        "            ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "        preprocessor = ColumnTransformer([\n",
        "            (\"num\", RobustScaler(), numeric_cols),  # More robust to outliers\n",
        "            (\"cat\", ohe, categorical_cols)\n",
        "        ])\n",
        "\n",
        "        return preprocessor\n",
        "\n",
        "    @staticmethod\n",
        "    def create_model(model_type: str, random_state: int = 42) -> Any:\n",
        "        \"\"\"Create model based on type\"\"\"\n",
        "        models = {\n",
        "            'gradient_boosting': GradientBoostingRegressor(\n",
        "                random_state=random_state,\n",
        "                learning_rate=0.05,\n",
        "                n_estimators=500,\n",
        "                max_depth=4,\n",
        "                subsample=0.8,\n",
        "                validation_fraction=0.1,\n",
        "                n_iter_no_change=50\n",
        "            ),\n",
        "            'random_forest': RandomForestRegressor(\n",
        "                random_state=random_state,\n",
        "                n_estimators=300,\n",
        "                max_depth=8,\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=4,\n",
        "                bootstrap=True,\n",
        "                oob_score=True\n",
        "            ),\n",
        "            'ridge': Ridge(alpha=1.0),\n",
        "            'elastic_net': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=random_state)\n",
        "        }\n",
        "\n",
        "        return models.get(model_type, models['gradient_boosting'])\n",
        "\n",
        "    @staticmethod\n",
        "    def get_hyperparameter_grid(model_type: str) -> Dict[str, List]:\n",
        "        \"\"\"Get hyperparameter grid for model tuning\"\"\"\n",
        "        grids = {\n",
        "            'gradient_boosting': {\n",
        "                'model__learning_rate': [0.01, 0.05, 0.1],\n",
        "                'model__max_depth': [3, 4, 6],\n",
        "                'model__n_estimators': [300, 500],\n",
        "                'model__subsample': [0.8, 0.9]\n",
        "            },\n",
        "            'random_forest': {\n",
        "                'model__n_estimators': [200, 300],\n",
        "                'model__max_depth': [6, 8, 10],\n",
        "                'model__min_samples_split': [5, 10],\n",
        "                'model__min_samples_leaf': [2, 4]\n",
        "            },\n",
        "            'ridge': {\n",
        "                'model__alpha': [0.1, 1.0, 10.0, 100.0]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return grids.get(model_type, {})\n",
        "\n",
        "\n",
        "class AdvancedROIEngine:\n",
        "    \"\"\"Main ROI prediction engine with advanced capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, config: ROIEngineConfig, logger: logging.Logger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.feature_engineer = FeatureEngineer(config, logger)\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def load_or_generate_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load existing data or generate new synthetic data\"\"\"\n",
        "        if self.config.data_path.exists():\n",
        "            self.logger.info(f\"Loading existing data from {self.config.data_path}\")\n",
        "            df = pd.read_csv(self.config.data_path)\n",
        "            df[self.config.date_col] = pd.to_datetime(df[self.config.date_col])\n",
        "        else:\n",
        "            self.logger.info(\"Generating new synthetic dataset\")\n",
        "            generator = SyntheticDataGenerator(self.config, self.logger)\n",
        "            df = generator.generate_dataset(n_periods=120)\n",
        "            df.to_csv(self.config.data_path, index=False)\n",
        "            self.logger.info(f\"Saved synthetic data to {self.config.data_path}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
        "        \"\"\"Prepare data with feature engineering\"\"\"\n",
        "        # Apply feature engineering\n",
        "        df_processed = self.feature_engineer.engineer_features(df)\n",
        "\n",
        "        # Identify feature columns\n",
        "        exclude_cols = [self.config.target, self.config.date_col, \"value_next\", \"year_month\"]\n",
        "\n",
        "        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        numeric_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
        "\n",
        "        categorical_cols = [c for c in self.config.id_cols if c in df_processed.columns]\n",
        "        for c in df_processed.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
        "            if c not in [self.config.target, self.config.date_col] and c not in categorical_cols:\n",
        "                categorical_cols.append(c)\n",
        "\n",
        "        self.logger.info(f\"Prepared {len(numeric_cols)} numeric and {len(categorical_cols)} categorical features\")\n",
        "\n",
        "        return df_processed, numeric_cols, categorical_cols\n",
        "\n",
        "    def train_single_model(self, X: pd.DataFrame, y: np.ndarray,\n",
        "                          model_type: str, numeric_cols: List[str],\n",
        "                          categorical_cols: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"Train a single model with cross-validation\"\"\"\n",
        "\n",
        "        # Create preprocessing and model pipeline\n",
        "        preprocessor = ModelFactory.create_preprocessor(numeric_cols, categorical_cols)\n",
        "        model = ModelFactory.create_model(model_type, self.config.random_state)\n",
        "        pipeline = Pipeline([(\"preprocess\", preprocessor), (\"model\", model)])\n",
        "\n",
        "        # Hyperparameter tuning if enabled\n",
        "        if self.config.enable_hyperparameter_tuning:\n",
        "            param_grid = ModelFactory.get_hyperparameter_grid(model_type)\n",
        "            if param_grid:\n",
        "                self.logger.info(f\"Performing hyperparameter tuning for {model_type}\")\n",
        "\n",
        "                # Use fewer splits for tuning to speed up\n",
        "                tscv_tune = TimeSeriesSplit(n_splits=3)\n",
        "                grid_search = GridSearchCV(\n",
        "                    pipeline, param_grid, cv=tscv_tune,\n",
        "                    scoring='neg_mean_squared_error',\n",
        "                    n_jobs=self.config.n_jobs, verbose=1\n",
        "                )\n",
        "                grid_search.fit(X, y)\n",
        "                pipeline = grid_search.best_estimator_\n",
        "                self.logger.info(f\"Best parameters for {model_type}: {grid_search.best_params_}\")\n",
        "\n",
        "        # Cross-validation evaluation\n",
        "        tscv = TimeSeriesSplit(n_splits=self.config.n_splits)\n",
        "        cv_scores = {'mae': [], 'rmse': [], 'r2': [], 'mape': []}\n",
        "        y_true_all, y_pred_all = [], []\n",
        "\n",
        "        for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
        "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "            # Fit pipeline\n",
        "            pipeline.fit(X_train, y_train)\n",
        "            y_pred = pipeline.predict(X_test)\n",
        "\n",
        "            # Calculate metrics (MODIFIED: Use np.sqrt for RMSE to avoid 'squared' parameter issue)\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            rmse = np.sqrt(mse)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            mape = np.mean(np.abs((y_test - y_pred) / np.clip(np.abs(y_test), 1e-6, None))) * 100\n",
        "\n",
        "            cv_scores['mae'].append(mae)\n",
        "            cv_scores['rmse'].append(rmse)\n",
        "            cv_scores['r2'].append(r2)\n",
        "            cv_scores['mape'].append(mape)\n",
        "\n",
        "            y_true_all.extend(y_test.tolist())\n",
        "            y_pred_all.extend(y_pred.tolist())\n",
        "\n",
        "            self.logger.info(f\"{model_type} Fold {fold_idx}: MAE={mae:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
        "\n",
        "        # Fit final model on all data\n",
        "        pipeline.fit(X, y)\n",
        "\n",
        "        # Calculate average metrics\n",
        "        avg_metrics = {metric: np.mean(scores) for metric, scores in cv_scores.items()}\n",
        "        std_metrics = {metric: np.std(scores) for metric, scores in cv_scores.items()}\n",
        "\n",
        "        return {\n",
        "            'pipeline': pipeline,\n",
        "            'cv_metrics': cv_scores,\n",
        "            'avg_metrics': avg_metrics,\n",
        "            'std_metrics': std_metrics,\n",
        "            'y_true': y_true_all,\n",
        "            'y_pred': y_pred_all,\n",
        "            'model_type': model_type\n",
        "        }\n",
        "\n",
        "    def train_models(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Train multiple models and compare performance\"\"\"\n",
        "        self.logger.info(\"Starting model training...\")\n",
        "\n",
        "        # Prepare data\n",
        "        df_processed, numeric_cols, categorical_cols = self.prepare_data(df)\n",
        "\n",
        "        # Sort by date for proper time series splitting\n",
        "        df_processed = df_processed.sort_values(self.config.date_col).reset_index(drop=True)\n",
        "\n",
        "        X = df_processed.drop(columns=[self.config.target])\n",
        "        y = df_processed[self.config.target].values\n",
        "\n",
        "        self.logger.info(f\"Training dataset shape: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "        # Train each model\n",
        "        results = {}\n",
        "        for model_type in self.config.models_to_try:\n",
        "            self.logger.info(f\"Training {model_type} model...\")\n",
        "\n",
        "            try:\n",
        "                model_results = self.train_single_model(\n",
        "                    X, y, model_type, numeric_cols, categorical_cols\n",
        "                )\n",
        "                results[model_type] = model_results\n",
        "\n",
        "                avg_mae = model_results['avg_metrics']['mae']\n",
        "                avg_r2 = model_results['avg_metrics']['r2']\n",
        "                self.logger.info(f\"{model_type} completed - Average MAE: {avg_mae:.4f}, R²: {avg_r2:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error training {model_type}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Select best model based on R²\n",
        "        if results:\n",
        "            best_model = max(results.keys(), key=lambda k: results[k]['avg_metrics']['r2'])\n",
        "            self.logger.info(f\"Best model: {best_model} (R² = {results[best_model]['avg_metrics']['r2']:.4f})\")\n",
        "\n",
        "            # Store best model\n",
        "            self.best_model_name = best_model\n",
        "            self.best_model = results[best_model]['pipeline']\n",
        "\n",
        "        self.results = results\n",
        "        return results\n",
        "\n",
        "    def create_visualizations(self):\n",
        "        \"\"\"Create comprehensive visualization suite\"\"\"\n",
        "        if not self.results:\n",
        "            self.logger.warning(\"No results available for visualization\")\n",
        "            return\n",
        "\n",
        "        self.logger.info(\"Creating visualizations...\")\n",
        "\n",
        "        # Set up plotting style\n",
        "        plt.style.use('seaborn-v0_8')\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "\n",
        "        # 1. Model Comparison\n",
        "        self._plot_model_comparison()\n",
        "\n",
        "        # 2. Feature Importance (for best model)\n",
        "        if hasattr(self, 'best_model'):\n",
        "            self._plot_feature_importance()\n",
        "\n",
        "        # 3. Residual Analysis\n",
        "        self._plot_residual_analysis()\n",
        "\n",
        "        # 4. Prediction vs Actual\n",
        "        self._plot_predictions()\n",
        "\n",
        "        # 5. Time Series Analysis\n",
        "        self._plot_time_series_performance()\n",
        "\n",
        "        self.logger.info(\"Visualizations completed\")\n",
        "\n",
        "    def _plot_model_comparison(self):\n",
        "        \"\"\"Plot model performance comparison\"\"\"\n",
        "        if len(self.results) < 2:\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "        models = list(self.results.keys())\n",
        "        metrics = ['mae', 'rmse', 'r2', 'mape']\n",
        "        metric_names = ['Mean Absolute Error', 'Root Mean Squared Error', 'R² Score', 'Mean Absolute Percentage Error']\n",
        "\n",
        "        for idx, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "            ax = axes[idx//2, idx%2]\n",
        "\n",
        "            means = [self.results[model]['avg_metrics'][metric] for model in models]\n",
        "            stds = [self.results[model]['std_metrics'][metric] for model in models]\n",
        "\n",
        "            bars = ax.bar(models, means, yerr=stds, capsize=5, alpha=0.8)\n",
        "            ax.set_title(name, fontweight='bold')\n",
        "            ax.set_ylabel(name)\n",
        "\n",
        "            # Rotate x-axis labels if needed\n",
        "            if len(max(models, key=len)) > 8:\n",
        "                ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for bar, mean in zip(bars, means):\n",
        "                height = bar.get_height()\n",
        "                ax.annotate(f'{mean:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                           xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.config.plots_dir / f'model_comparison.{self.config.plot_format}',\n",
        "                   dpi=self.config.plot_dpi, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_feature_importance(self):\n",
        "        \"\"\"Plot feature importance for the best model\"\"\"\n",
        "        if not hasattr(self, 'best_model'):\n",
        "            return\n",
        "\n",
        "        # Extract model from pipeline\n",
        "        model = self.best_model.named_steps['model']\n",
        "        preprocessor = self.best_model.named_steps['preprocess']\n",
        "\n",
        "        if not hasattr(model, 'feature_importances_'):\n",
        "            return\n",
        "\n",
        "        # Get feature names\n",
        "        numeric_features = preprocessor.transformers_[0][2]\n",
        "        cat_encoder = preprocessor.transformers_[1][1]\n",
        "        cat_features = preprocessor.transformers_[1][2]\n",
        "\n",
        "        if hasattr(cat_encoder, 'get_feature_names_out'):\n",
        "            cat_feature_names = cat_encoder.get_feature_names_out(cat_features).tolist()\n",
        "        else:\n",
        "            cat_feature_names = []\n",
        "\n",
        "        feature_names = list(numeric_features) + cat_feature_names\n",
        "        importances = model.feature_importances_\n",
        "\n",
        "        # Sort by importance\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "        top_n = min(20, len(indices))\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.bar(range(top_n), importances[indices[:top_n]], alpha=0.8)\n",
        "        plt.title(f'Top {top_n} Feature Importances - {self.best_model_name.title()}',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Features')\n",
        "        plt.ylabel('Importance')\n",
        "\n",
        "        feature_labels = [feature_names[i] if i < len(feature_names) else f'Feature_{i}'\n",
        "                         for i in indices[:top_n]]\n",
        "        plt.xticks(range(top_n), feature_labels, rotation=90)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.config.plots_dir / f'feature_importance.{self.config.plot_format}',\n",
        "                   dpi=self.config.plot_dpi, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_residual_analysis(self):\n",
        "        \"\"\"Create residual analysis plots\"\"\"\n",
        "        if not hasattr(self, 'best_model_name'):\n",
        "            return\n",
        "\n",
        "        best_results = self.results[self.best_model_name]\n",
        "        y_true = np.array(best_results['y_true'])\n",
        "        y_pred = np.array(best_results['y_pred'])\n",
        "        residuals = y_true - y_pred\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle(f'Residual Analysis - {self.best_model_name.title()}',\n",
        "                    fontsize=16, fontweight='bold')\n",
        "\n",
        "        # 1. Residuals vs Predicted\n",
        "        axes[0,0].scatter(y_pred, residuals, alpha=0.6, s=20)\n",
        "        axes[0,0].axhline(y=0, color='red', linestyle='--')\n",
        "        axes[0,0].set_xlabel('Predicted Values')\n",
        "        axes[0,0].set_ylabel('Residuals')\n",
        "        axes[0,0].set_title('Residuals vs Predicted')\n",
        "\n",
        "        # 2. Q-Q Plot\n",
        "        from scipy import stats\n",
        "        stats.probplot(residuals, dist=\"norm\", plot=axes[0,1])\n",
        "        axes[0,1].set_title('Q-Q Plot (Normal Distribution)')\n",
        "\n",
        "        # 3. Histogram of Residuals\n",
        "        axes[1,0].hist(residuals, bins=30, alpha=0.7, density=True)\n",
        "        axes[1,0].set_xlabel('Residuals')\n",
        "        axes[1,0].set_ylabel('Density')\n",
        "        axes[1,0].set_title('Distribution of Residuals')\n",
        "\n",
        "        # Add normal curve overlay\n",
        "        mu, sigma = stats.norm.fit(residuals)\n",
        "        x = np.linspace(residuals.min(), residuals.max(), 100)\n",
        "        axes[1,0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', lw=2, label='Normal Fit')\n",
        "        axes[1,0].legend()\n",
        "\n",
        "        # 4. Scale-Location Plot\n",
        "        sqrt_abs_resid = np.sqrt(np.abs(residuals))\n",
        "        axes[1,1].scatter(y_pred, sqrt_abs_resid, alpha=0.6, s=20)\n",
        "        axes[1,1].set_xlabel('Predicted Values')\n",
        "        axes[1,1].set_ylabel('√|Residuals|')\n",
        "        axes[1,1].set_title('Scale-Location Plot')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.config.plots_dir / f'residual_analysis.{self.config.plot_format}',\n",
        "                   dpi=self.config.plot_dpi, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_predictions(self):\n",
        "        \"\"\"Plot predictions vs actual values\"\"\"\n",
        "        if not hasattr(self, 'best_model_name'):\n",
        "            return\n",
        "\n",
        "        best_results = self.results[self.best_model_name]\n",
        "        y_true = np.array(best_results['y_true'])\n",
        "        y_pred = np.array(best_results['y_pred'])\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        # Scatter plot\n",
        "        plt.scatter(y_true, y_pred, alpha=0.6, s=20)\n",
        "\n",
        "        # Perfect prediction line\n",
        "        min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "\n",
        "        # Calculate R²\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        plt.text(0.05, 0.95, f'R² = {r2:.4f}', transform=plt.gca().transAxes,\n",
        "                fontsize=12, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.xlabel('Actual Values')\n",
        "        plt.ylabel('Predicted Values')\n",
        "        plt.title(f'Predictions vs Actual - {self.best_model_name.title()}',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.config.plots_dir / f'predictions_vs_actual.{self.config.plot_format}',\n",
        "                   dpi=self.config.plot_dpi, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_time_series_performance(self):\n",
        "        \"\"\"Plot model performance over time\"\"\"\n",
        "        # This would require additional date tracking in cross-validation\n",
        "        # For now, we'll create a placeholder\n",
        "        pass\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save all results and models\"\"\"\n",
        "        self.logger.info(\"Saving results...\")\n",
        "\n",
        "        # Save model comparison metrics\n",
        "        comparison_data = []\n",
        "        for model_name, results in self.results.items():\n",
        "            row = {'model': model_name}\n",
        "            row.update(results['avg_metrics'])\n",
        "            row.update({f'{k}_std': v for k, v in results['std_metrics'].items()})\n",
        "            comparison_data.append(row)\n",
        "\n",
        "        comparison_df = pd.DataFrame(comparison_data)\n",
        "        comparison_df.to_csv(self.config.artifacts_dir / 'model_comparison.csv', index=False)\n",
        "\n",
        "        # Save detailed results\n",
        "        with open(self.config.artifacts_dir / 'detailed_results.json', 'w') as f:\n",
        "            # Convert results to JSON-serializable format\n",
        "            json_results = {}\n",
        "            for model_name, results in self.results.items():\n",
        "                json_results[model_name] = {\n",
        "                    'cv_metrics': results['cv_metrics'],\n",
        "                    'avg_metrics': results['avg_metrics'],\n",
        "                    'std_metrics': results['std_metrics'],\n",
        "                    'model_type': results['model_type']\n",
        "                    # Note: pipeline and predictions are not JSON serializable\n",
        "                }\n",
        "            json.dump(json_results, f, indent=2)\n",
        "\n",
        "        # Save best model\n",
        "        if hasattr(self, 'best_model'):\n",
        "            joblib.dump(self.best_model, self.config.artifacts_dir / 'best_model.joblib')\n",
        "\n",
        "            # Save predictions from best model\n",
        "            best_results = self.results[self.best_model_name]\n",
        "            predictions_df = pd.DataFrame({\n",
        "                'y_true': best_results['y_true'],\n",
        "                'y_pred': best_results['y_pred'],\n",
        "                'residuals': np.array(best_results['y_true']) - np.array(best_results['y_pred'])\n",
        "            })\n",
        "            predictions_df.to_csv(self.config.artifacts_dir / 'best_model_predictions.csv', index=False)\n",
        "\n",
        "        self.logger.info(f\"Results saved to {self.config.artifacts_dir}\")\n",
        "\n",
        "    def run_complete_analysis(self) -> Dict[str, Any]:\n",
        "        \"\"\"Run the complete ROI analysis pipeline\"\"\"\n",
        "        self.logger.info(\"Starting complete ROI analysis...\")\n",
        "\n",
        "        try:\n",
        "            # Load or generate data\n",
        "            df = self.load_or_generate_data()\n",
        "\n",
        "            # Train models\n",
        "            results = self.train_models(df)\n",
        "\n",
        "            # Create visualizations\n",
        "            if self.config.save_plots:\n",
        "                self.create_visualizations()\n",
        "\n",
        "            # Save results\n",
        "            self.save_results()\n",
        "\n",
        "            # Print summary\n",
        "            self._print_summary()\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in analysis pipeline: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _print_summary(self):\n",
        "        \"\"\"Print analysis summary\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ROI ENGINE ANALYSIS SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        if not self.results:\n",
        "            print(\"No results available.\")\n",
        "            return\n",
        "\n",
        "        # Model comparison\n",
        "        print(\"\\nModel Performance Comparison:\")\n",
        "        print(\"-\" * 40)\n",
        "        for model_name, results in self.results.items():\n",
        "            metrics = results['avg_metrics']\n",
        "            print(f\"{model_name.upper():<20} | R²: {metrics['r2']:.4f} | MAE: {metrics['mae']:.4f} | RMSE: {metrics['rmse']:.4f}\")\n",
        "\n",
        "        # Best model details\n",
        "        if hasattr(self, 'best_model_name'):\n",
        "            print(f\"\\nBest Model: {self.best_model_name.upper()}\")\n",
        "            best_metrics = self.results[self.best_model_name]['avg_metrics']\n",
        "            print(f\"  R² Score: {best_metrics['r2']:.4f}\")\n",
        "            print(f\"  Mean Absolute Error: {best_metrics['mae']:.4f}\")\n",
        "            print(f\"  Root Mean Squared Error: {best_metrics['rmse']:.4f}\")\n",
        "            print(f\"  Mean Absolute Percentage Error: {best_metrics['mape']:.2f}%\")\n",
        "\n",
        "        print(f\"\\nResults saved to: {self.config.artifacts_dir}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5) Main Execution\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Initialize configuration\n",
        "    config = ROIEngineConfig(\n",
        "        enable_advanced_features=True,\n",
        "        enable_hyperparameter_tuning=True,\n",
        "        models_to_try=['gradient_boosting', 'random_forest', 'ridge'],\n",
        "        save_plots=True\n",
        "    )\n",
        "\n",
        "    # Setup logging\n",
        "    logger = setup_logging(config)\n",
        "    logger.info(\"Starting Advanced ROI Engine\")\n",
        "\n",
        "    try:\n",
        "        # Initialize and run engine\n",
        "        engine = AdvancedROIEngine(config, logger)\n",
        "        results = engine.run_complete_analysis()\n",
        "\n",
        "        logger.info(\"Analysis completed successfully\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Analysis failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6) Execution\n",
        "# =========================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1ABVTP8aTYd",
        "outputId": "a0d6c081-e681-438b-e8f7-c62a96ae0b35"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "\n",
            "============================================================\n",
            "ROI ENGINE ANALYSIS SUMMARY\n",
            "============================================================\n",
            "\n",
            "Model Performance Comparison:\n",
            "----------------------------------------\n",
            "GRADIENT_BOOSTING    | R²: -0.3323 | MAE: 0.0144 | RMSE: 0.0166\n",
            "RANDOM_FOREST        | R²: -1.0932 | MAE: 0.0165 | RMSE: 0.0184\n",
            "RIDGE                | R²: 0.0479 | MAE: 0.0109 | RMSE: 0.0124\n",
            "\n",
            "Best Model: RIDGE\n",
            "  R² Score: 0.0479\n",
            "  Mean Absolute Error: 0.0109\n",
            "  Root Mean Squared Error: 0.0124\n",
            "  Mean Absolute Percentage Error: 13.92%\n",
            "\n",
            "Results saved to: roi_engine_project/artifacts\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}